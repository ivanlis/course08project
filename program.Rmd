---
title: "Workout Correctness Prediction"
author: "Ivan Lysiuchenko"
date: "July 15, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Data Preprocessing

It can be seen that some columns of the training dataset contain too
many NA values, so they will not help us in building the prediction
model. We omit the columns having at least 30% NA.
Also we remove the columns that don't describe any parameter of how
the excercise was done (user name, timestamps, etc.)

```{r message=FALSE}

# Read the data
trainingOriginal <- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!"))
testingOriginal <- read.csv("pml-testing.csv", na.strings = c("NA", "#DIV/0!"))


# Remove columns with too many na values
selectedColumns <- c()
naProportions <- c()

for (i in 1:ncol(trainingOriginal))
{
    naProp <- sum(is.na(trainingOriginal[, i])) / nrow(trainingOriginal)
    if (naProp < 0.3)
    {
        selectedColumns <- c(selectedColumns, i)
        naProportions <- c(naProportions, naProp)
    }
}

trainingFiltered <- trainingOriginal[, selectedColumns]

# Remove the columns which are irrelevant to our prediction
trainingFiltered <- trainingFiltered[ , 
                      !(names(trainingFiltered) %in% c(  c("X", "user_name", 
                                                   "raw_timestamp_part_1",
                                                   "raw_timestamp_part_2",
                                                   "cvtd_timestamp", "new_window",
                                                   "num_window")     
                                                 ))   ]
```

### Building the model

The problem we are dealing with is a classification problem. The Random Forest methods
are among the best for this kind of tasks. Let's fit a model using the filtered training
dataset.

```{r message=FALSE}
# Fit a random forest
library(randomForest)
modFitRf <- randomForest(classe ~ ., data = trainingFiltered)
```

TODO: some graphs describing the model

### Cross-validation and out-of-sample error estimation

```{r echo=FALSE}
cvP = 0.1
cvN = 19
```

To see if our choice is acceptable we cross validate our model withholding 
`r cvP * 100`% of data at each of `r cvN` iterations.

```{r message=FALSE}
# Cross validate our model
library(rfUtilities)
rfCrossValid <- rf.crossValidation(modFitRf, 
                                   trainingFiltered[, 
                                                    !(names(trainingFiltered) == "classe")],
                                   p = cvP, n = cvN)

cv2 <- rfcv(trainingFiltered[, !(names(trainingFiltered) == "classe")], trainingFiltered$classe, cv.fold = 10)


# The OOB error as an estimation of the out-of-sample error
```

One of the cross validation results is the cross validation out-of-bag (OOB) error.
It can be taken as an estimate for the out-of-sample error, i.e. the error when
the outcome is predicted on data not used to train the model.

TODO: give the cross validated OOB error value; some cross validation graphs.

The ten most important variables are:

```{r echo=FALSE}
data.frame(names = names(trainingFiltered)[order(-modFitRf$importance)], measure = modFitRf$importance[order(-modFitRf$importance)])[1:10,]
```


### Predicting on the test dataset

Let's use the built model to predict the classes for the `r nrow(testingOriginal)`
test cases.

```{r}
predictRfTest <- predict(modFitRf, testingOriginal)
predictRfTest
```
