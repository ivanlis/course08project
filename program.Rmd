---
title: "Workout Correctness Prediction"
author: "Ivan Lysiuchenko"
date: "July 20, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Data Preprocessing

It can be seen that some columns of the training dataset contain too
many NA values, so they will not help us in building the prediction
model. We omit the columns having at least 30% NA.
Also we remove the columns that don't describe any parameter of how
the excercise was done (user name, timestamps, etc.)

```{r message=FALSE}

# Read the data
trainingOriginal <- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!"))
testingOriginal <- read.csv("pml-testing.csv", na.strings = c("NA", "#DIV/0!"))


# Remove columns with too many na values
selectedColumns <- c()
naProportions <- c()

for (i in 1:ncol(trainingOriginal))
{
    naProp <- sum(is.na(trainingOriginal[, i])) / nrow(trainingOriginal)
    if (naProp < 0.3)
    {
        selectedColumns <- c(selectedColumns, i)
        naProportions <- c(naProportions, naProp)
    }
}

trainingFiltered <- trainingOriginal[, selectedColumns]

# Remove the columns which are irrelevant to our prediction
trainingFiltered <- trainingFiltered[ , 
                      !(names(trainingFiltered) %in% c(  c("X", "user_name", 
                                                   "raw_timestamp_part_1",
                                                   "raw_timestamp_part_2",
                                                   "cvtd_timestamp", "new_window",
                                                   "num_window")     
                                                 ))   ]
```

If we try to represent graphically the dependence of the outcome on individual predictors, it's 
not too clear how we can describe it. It can be seen on the following plot where the colors
represent the way the excercise was done as a function of two of the predictor variables.

```{r}
library(ggplot2)
g <- ggplot(data = trainingFiltered, 
            mapping = aes(x = roll_belt, y = yaw_belt, color = classe)) + geom_point()
```

### Building the model

The problem we are dealing with is a classification problem. The Random Forest methods
are among the best for this kind of tasks. Let's fit a model using the filtered training
dataset.

```{r message=FALSE}
# Fit a random forest
library(randomForest)
modFitRf <- randomForest(classe ~ ., data = trainingFiltered)
```


### Cross-validation and out-of-sample error estimation

```{r echo=FALSE}
cvP = 0.1
cvN = 19
```

To see if our choice is acceptable we cross validate our model withholding 
`r cvP * 100`% of data at each of `r cvN` iterations.

```{r message=FALSE}
# Cross validate our model
library(rfUtilities)
#rfCrossValid <- rf.crossValidation(modFitRf, 
#                                   trainingFiltered[, 
#                                                    !(names(trainingFiltered) =="classe")],
#                                   p = cvP, n = cvN)

cv2 <- rfcv(trainingFiltered[, !(names(trainingFiltered) == "classe")], trainingFiltered$classe, cv.fold = 10)

```

We're using all the 52 variables as predictors. The cross validation error is 
`r cv2$error.cv[1]`. It can be considered an estimate of the out-of-sample error.

The following graph shows the cross validation error as a function of the number of
variables taken as predictors.

```{r}
g <- ggplot(data = data.frame(x = cv2$n.var, y = cv2$error.cv), 
            mapping = aes(x = x, y = y)) + 
    geom_line() + geom_point() + 
    labs(x = "Number of variables", y = "Cross validation error", 
         title = "Cross validation error depending on the number of variables")
```

The ten most important variables are:

```{r echo=FALSE}
data.frame(names = names(trainingFiltered)[order(-modFitRf$importance)], 
           measure = modFitRf$importance[order(-modFitRf$importance)])[1:10,]
```


### Predicting on the test dataset

Let's use the built model to predict the classes for the `r nrow(testingOriginal)`
test cases.

```{r}
predictRfTest <- predict(modFitRf, testingOriginal)
predictRfTest
```
